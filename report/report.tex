\documentclass[a4paper]{acmtrans2m}

\usepackage{url}

\markboth{CS497}{Some Recent Advances in Haskell}

\title{CS497 -- Some Recent Advances in Haskell}
\author{Siddharth Agarwal | Y7429\\
  Advisor: Dr. Amey Karkare}
\date{\today}

\begin{abstract}
We look at two recently developed software engineering tools currently available
in Haskell: software transactional memory (along with an implementation in an
imperative language for comparison), and Haskell's nested data parallel library,
currently in development. We pay special attention to how Haskell's unique
features --- in particular, the static separation of functions with side effects
from functions without --- help in making these implementations easier and
faster.
\end{abstract}

% A dummy category to please the cls file
\category{Foo}{Bar}{Baz}

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}
The computing world today is multi-core. Processor speeds have reached their
limit, so the only way they can become faster now is by including multiple
processing units on one chip. The upshot of this is that the problem of
concurrent programming is now more relevant than ever.

We look at the problem from two different angles:
\begin{itemize}
\item Section \ref{sec:stm} will look at solving problems where threads have
  significant amounts of shared state that is concurrently accessed and mutated.
\item Section \ref{sec:dph} will look at cases where the algorithm does not
  require much state to be shared among threads, but otherwise accesses data in
  non-trivial ways.
\end{itemize}

\section{Software transactional memory}
\label{sec:stm}

\subsection{Motivation}

Concurrent programming brings several problems along with it, of which the
biggest one is the synchronization of access to \textit{shared mutable
  state}. The dominant form of synchronization today is using locks.

Locks are structures with the invariant that only one thread can hold one at
a given instant. They are a powerful tool but can be very difficult to program
with. Programmers must ensure that operations do not conflict, do not deadlock,
and that the locking is of the right granularity \cite{Duffy:2010}. In addition,
locks are not composable: in general, it is not possible to combine thread-safe
sections of code into a larger thread-safe section of code \cite{Harris:2005}.

\subsection{Transactional memory systems}
To combat these issues, several authors have proposed memory systems where the
fundamental unit of synchronization is the \textit{transaction}
\cite{Harris:2007}. Transactions, as a concept borrowed from databases, are
sections of code in which the load and store operations appear to be atomic (all
happen at once). The earliest transactional memory systems
(e.g. \cite{Herlihy:1993}) all required specialized hardware support, but
\textit{software transactional memory (STM)}, which required no hardware support
beyond the \textit{load-linked/store-conditional} construct \cite{Shavit:1995}, or
equivalently the \textit{compare-and-swap} operation \cite{Harris:2003}, quickly
arose.

\subsection{Language support}

Later, \citeN{Harris:2003} described a system based on critical conditional
regions (CCR) \cite{Hoare:1972} where transaction support was integrated into
the Java language syntax. Such systems, while quite successful, still suffered
from a few problems \cite{Harris:2005,Duffy:2010}:

\begin{itemize}
\item \textbf{Overhead.} To avoid A-B-A problems, each location of transactional
  memory needs a version number. Since these systems proposed to make all memory
  transactional, a naive implementation would mean twice the memory usage. A few
  solutions were proposed, including having a common version number for a group
  of words instead of a separate version number for each word
  \cite{Harris:2003}.
\item \textbf{I/O.} I/O operations are generally irreversible, which causes
  problems with transactions (since they might need to roll back). For
  output-only operations, one might be able to buffer them until one knows the
  transaction is successful, but that wouldn't work for operations which require
  input. A different solution is to forbid any I/O operations, but languages
  like Java do not have any static information about which functions have I/O,
  so they must detect such cases and abort at run-time.
\item \textbf{Weak vs. strong atomicity.} These systems did not restrict the
  same memory location from being accessed both inside and outside transactional
  sections. One could either guarantee \textit{strong} transactional semantics
  outside transactional blocks too (which is equivalent to wrapping every
  non-transactional statement in a one-line transactional block and leads to
  poor performance), or be \textit{weakly} atomic and make no such
  guarantees. One could also avert the problem by making the mutable memory
  accessible inside transactional blocks \textit{disjoint} from mutable memory
  accessible outside them.
\end{itemize}

\subsection{STM in Haskell}

Haskell, as a statically-typed pure functional language with limited and
controlled mutable state, is uniquely suited to software transactions
\cite{Harris:2005}. In Haskell, any mutation of state has to be done inside a
function whose type contains the \texttt{IO} monad. What \citeN{Harris:2005}
propose is a monad for STM operations, called \texttt{STM}.

\begin{itemize}
\item To minimize \textbf{overhead}, they require that any transactional memory
  explicitly be declared as \texttt{TVar}. This means that only a very small
  number of memory locations are transactional, and that each location can contain
  a separate version number while still not adding much overhead.
\item To solve the \textbf{I/O} problem, they forbid general I/O operations inside
  any function marked \texttt{STM}. In Haskell, all I/O operations are marked with
  the \texttt{IO} monad, and since Haskell is statically-typed, the compiler knows
  about these operations. This means that I/O can be statically forbidden inside
  \texttt{STM} sections.
\item To avert the \textbf{weak vs. strong atomicity} problem, they do not allow
  transactional memory to be accessed outside \texttt{STM} blocks. All
  transactional memory must be accessed using the pair of functions
  \texttt{readTVar} and \texttt{writeTVar}, and since these functions are marked
  as \texttt{STM}, they can only be called inside \texttt{STM} blocks.
\end{itemize}

\subsection{Current status and future directions}

Haskell's STM system currently ships with releases of the Glasgow Haskell compiler (GHC).
It is perhaps the only STM to now be mature enough to be used in production systems
\cite{Stewart:2009}. However, work still remains to be done in integrating memory
transactions with other sorts of transactional providers, such as databases and
transactional file systems \cite{SPJ:2006}.

\section{Nested data parallelism}
\label{sec:dph}

\subsection{Motivation}
A different approach to concurrent programming, suitable for certain kinds of
applications, is via \textit{data parallelism}. Data parallelism is
characterized by large sets of data on which transformations need to be done,
and by the fact that these transformations can be done on individual data
units mostly independently. Such parallelism can be used in a wide variety of
places, from calculating the dot product of a pair of vectors to blending
images.

However, traditionally data parallelism libraries require the data to be stored
as flat arrays. This is fine for simple applications like blending images, but
can be quite inconvenient to the programmer for anything more complicated.

\subsection{Early attempts}
One possible way to help the programmer to exploit data parallelism would be to
allow nested data structures to be provided, transform them into flat structures,
operate on them and finally convert them back to nested structures. Indeed, such a solution was proposed in the early '90s \cite{Blelloch:1993,Blelloch:1996}.
However, this solution required the programmer to program in a special
language called NESL, and was limited in other ways \cite{Chakravarty:2007}.

\subsection{Data Parallel Haskell (DPH)}

\citeN{Chakravarty:2007} described a system which takes the key ideas from NESL,
generalizes them to apply in the larger context of a general-purpose functional
programming language, and implements them in Haskell. The key insight in NESL was
the vectorization transformation, which converts a nested data structure into flat
arrays and transforms functions that apply on these data structures and their
elements to apply on these arrays. Generalized to Haskell,
the vectorization transformation is able to operate on user-created unions and
higher-order functions \cite{SPJ:2008}.

It would be much more difficult to implement vectorizations in a non-pure
language than in Haskell, because the vectorization transformation assumes and
requires that the functions be pure. Since in Haskell impure functions are 
separated at compile-time, the vectorization operation performed by the compiler
on seeing such functions can raise compile-time errors.

\subsection{Current status}
Data Parallel Haskell as implemented in shipping versions of the Glasgow Haskell
compiler is still a work in progress. There are two backends, a sequential backend
(for reference) and a parallel backend. There are also two interfaces
provided: an "unlifted" interface, which works well but is equivalent to flat data
parallelism, and a "lifted" interface, which is capable of performing vectorization
but is currently quite buggy.

We attempted to implement a simple dot product operation using both the unlifted and
lifted interfaces, and ran into several problems with the lifted interface:

\begin{enumerate}
\item The normal operations \texttt{Prelude.+, Prelude.-, Prelude.*} etc aren't vectorizable, since DPH currently doesn't support Haskell type classes. The workaround
is to use the special operations that DPH provides, e.g. \texttt{Data. Array.Parallel.Prelude.Int.+} or \texttt{Float.+}.

\item Flat arrays of the type \texttt{[:Int:]} cannot yet be transferred between
vectorized and non-vectorized code. Instead one needs to use a wrapper function
that accepts \texttt{PArray Int}, and use \texttt{fromPArrayP} to
convert to an \texttt{[:Int:]}.

\item The optimizer tended to inline the wrapper function into non-vectorized
code. This is undesirable, since it really is the interface between vectorized and
non-vectorized code. The solution is to mark the function as \texttt{NOINLINE} 
\citeA{DPHRef}.

\item The enumeration function (\texttt{..} or \texttt{enumFromTo}) isn't
vectorizable. This means that the following order of instructions doesn't work in
purely vectorized code.

\begin{verbatim}
  a1 = dotp v1 v1
  a2 = dotp v2 v2
  v = [:a1..a2:]
  dotp v v
\end{verbatim}

\item This means that the only way to perform the above set of operations is to
first perform the individual dot products, then get their results back into
non-vectorized code, and finally move to vectorized code again. Unfortunately
this sequence of operations causes a stack overflow with both the parallel and
sequential versions of Data Parallel Haskell.

\item Because of (4) and (5), we decided to do an explicit enumeration instead 
of using enumFromTo. This worked with the sequential version of the library but not
the parallel version.
\end{enumerate}

We were finally able to get the following sequence of operations working with both
versions of the data parallel library:

\begin{verbatim}
  vsum := parallel term-by-term sum of two arrays
  wsum := parallel term-by-term sum of two different arrays
  result := sum over all elements of the term-by-term product of
            vsum and wsum
\end{verbatim}

With GHC 6.12.3 on Windows 7 with a Core i7 720QM processor, we were able to run the
program on arrays of 40 and 100 elements each. The speedup observed with 40-element
arrays was from 0.17 seconds with a single thread to 0.11 seconds with two threads.
Similarly, the speedup observed with 100-element arrays was from 3.75 seconds with a
single thread to 2.51 seconds with two threads. Of course, the lifted interface is
currently quite immature, and we saw the unlifted version perform much faster (less
than 0.05 seconds in each case).

\subsection{Future directions}

Current work is focused on eliminating bugs from the vectorizer and improving the 
performance of the parallel backend. There is also a plan to implement a CUDA-based
backend for Data Parallel Haskell \cite{Dijk:2008}, but that has challenges of its own --- for
instance, the set of primitive operations available on CUDA is much smaller than the 
interface provides \cite{Leshchinskiy:2008}.

\section{Conclusion}

Concurrent programming presents several challenges, and programmers cannot ignore them
any more. Haskell, with its unique features, can provide software engineering 
tools to make concurrent programming for large classes of problems easier than ever
before.

\bibliographystyle{acmtrans}
\bibliography{report}
\end{document}
